{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_equally_spaced_colors(k):\n",
    "    colors = []\n",
    "    step = 360 / k  # Equally spaced hue step\n",
    "\n",
    "    for i in range(k):\n",
    "        hue = i * step  # Equally spaced hue values\n",
    "        rgb = hsv_to_rgb(hue, 1, 1)  # Convert hue to RGB values\n",
    "        scaled_rgb = tuple(\n",
    "            int(val * 255) for val in rgb\n",
    "        )  # Scale RGB values to 0-255 range\n",
    "        colors.append(scaled_rgb)\n",
    "\n",
    "    return colors\n",
    "\n",
    "\n",
    "def hsv_to_rgb(h, s, v):\n",
    "    c = v * s\n",
    "    x = c * (1 - abs((h / 60) % 2 - 1))\n",
    "    m = v - c\n",
    "\n",
    "    if 0 <= h < 60:\n",
    "        rgb = (c, x, 0)\n",
    "    elif 60 <= h < 120:\n",
    "        rgb = (x, c, 0)\n",
    "    elif 120 <= h < 180:\n",
    "        rgb = (0, c, x)\n",
    "    elif 180 <= h < 240:\n",
    "        rgb = (0, x, c)\n",
    "    elif 240 <= h < 300:\n",
    "        rgb = (x, 0, c)\n",
    "    else:\n",
    "        rgb = (c, 0, x)\n",
    "\n",
    "    return tuple((val + m) for val in rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def generate_binary_mask(annotations, image_size):\n",
    "    # Create a blank image\n",
    "    image = Image.new('L', image_size, 0)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for ann in annotations:\n",
    "        if ann[\"label\"] != \"Wire\":\n",
    "            continue\n",
    "        segs = ann[\"segmentation\"]\n",
    "        for seg in segs:\n",
    "            extPoints = seg[\"extPoints\"]\n",
    "            intPoints = seg[\"intPoints\"]\n",
    "            if len(extPoints) < 2 and len(intPoints) < 2:\n",
    "                continue\n",
    "            intPoints = [list(map(tuple,int)) for int in intPoints]\n",
    "            extPoints = list(map(tuple,extPoints))\n",
    "            # Draw the exterior contour\n",
    "            draw.polygon(extPoints, outline=1, fill=1)\n",
    "\n",
    "            # Draw the interior contours\n",
    "            for interior_contour in intPoints:\n",
    "                draw.polygon(interior_contour, outline=0, fill=0)\n",
    "\n",
    "    # Convert the image to a binary mask (numpy array)\n",
    "    binary_mask = np.array(image)\n",
    "\n",
    "    return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wires.json\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_set = set()\n",
    "# for image in data[\"images\"]:\n",
    "#     url_set.add((image[\"url\"], image[\"id\"]))\n",
    "\n",
    "# image_set = set()\n",
    "# imageid_set = set()\n",
    "\n",
    "# for url, id in url_set:\n",
    "#     req = requests.get(\"http://\" + url)\n",
    "#     img = Image.open(BytesIO(req.content)).convert(\"RGB\").resize((448, 448))\n",
    "#     byte_img = img.tobytes()\n",
    "#     if byte_img not in image_set:\n",
    "#         image_set.add(byte_img)\n",
    "#         imageid_set.add(id)\n",
    "        \n",
    "# disclude_id_set = set(\n",
    "#     [\n",
    "#         \"64696be14d61f800078e5be9\",\n",
    "#         \"648b7c226f177e0007432879\",\n",
    "#         \"646c4baeeee7ce000765e791\",\n",
    "#         \"646c5784eee7ce00076678f9\",\n",
    "#         \"646c6335eee7ce0007670362\",\n",
    "#         \"646ce574eee7ce00076f6273\",\n",
    "#         \"646ce65665bbde0007f09705\",\n",
    "#         \"646ce5dce326cf0007b927f2\",\n",
    "#         \"646d9cae09affa0007c9a1f0\",\n",
    "#         \"646da0bf09affa0007c9cd35\",\n",
    "#         \"646dbe8b65bbde0007fe0640\",\n",
    "#         \"646df9dd09affa0007cea11e\",\n",
    "#         \"646e15bfe326cf0007ca4d5f\",\n",
    "#         \"646e185b4d61f80007ca4af1\",\n",
    "#         \"646e185ae326cf0007ca7d3f\",\n",
    "#         \"646e276109affa0007d1dab3\",\n",
    "#         \"646e325809affa0007d2c5ec\",\n",
    "#         \"646e361909affa0007d31a96\",\n",
    "#         \"646e372b4d61f80007cc9555\",\n",
    "#         \"646e37de09affa0007d341ca\",\n",
    "#         \"646e37d609affa0007d340e0\",\n",
    "#         \"646e3ea109affa0007d3cb65\",\n",
    "#         \"646ebed7e326cf0007d4e660\",\n",
    "#         \"646ed5ac4d61f80007d69cee\",\n",
    "#         \"646f6a304d61f80007def5ca\",\n",
    "#         \"647000e2cdccc800070edc63\",\n",
    "#         \"647023199999350007945f9d\",\n",
    "#         \"647058565ad8d50007eb67e9\",\n",
    "#         \"64715fa42687e40007e9fa99\",\n",
    "#         \"64716edb2687e40007eaa357\",\n",
    "#         \"647179542687e40007eb1590\",\n",
    "#         \"647185f2fa532900077861da\",\n",
    "#         \"6471ae9a2687e40007eddabc\",\n",
    "#         \"6471b3822687e40007ee10b3\",\n",
    "#         \"6471c9892687e40007ef465c\",\n",
    "#         \"6471e04ffa532900077c4ffc\",\n",
    "#         \"6474cead50ebc70007df89d7\",\n",
    "#         \"64755af42687e40007187283\",\n",
    "#         \"64687349eee7ce00073655c8\",\n",
    "# \"646882634d61f8000783b0f1\",\n",
    "# \"646c3d0feee7ce00076544da\",\n",
    "# \"646c4c97eee7ce000765f48f\",\n",
    "# \"646c72a565bbde0007e89b6c\",\n",
    "# \"646c7346eee7ce000767cca7\",\n",
    "# \"646cc34deee7ce00076c53f3\",\n",
    "# \"646cc69ce326cf0007b6b0de\",\n",
    "# \"646cdb89eee7ce00076e65e5\",\n",
    "# \"646ce0e565bbde0007f02c55\",\n",
    "# \"646ce56eeee7ce00076f6223\",\n",
    "# \"646d991a65bbde0007fc2c0d\",\n",
    "# \"646da40465bbde0007fcaebc\",\n",
    "# \"646da6f509affa0007ca141a\",\n",
    "# \"646dc01509affa0007cb4e9c\",\n",
    "# \"6476d3f97d099e000720e9c9\",\n",
    "# \"646dcd5865bbde0007fee2d2\",\n",
    "# \"646de2af09affa0007cd4764\",\n",
    "# \"646de35165bbde0007002d58\",\n",
    "# \"646df3c365bbde0007012a84\",\n",
    "# \"646e058f09affa0007cf4fee\",\n",
    "# \"646e11db09affa0007d011a3\",\n",
    "# \"646e188e09affa0007d09cca\",\n",
    "# \"646ec2bb4d61f80007d5cefb\",\n",
    "# \"646ecb05e326cf0007d56e94\",\n",
    "# \"646ed5704d61f80007d69904\",\n",
    "# \"646f5cad65bbde00071876a7\",\n",
    "# \"646f713c65bbde00071a3273\",\n",
    "# \"647005228f02f700070dcc1c\",\n",
    "# \"64700117cdccc800070edfd4\",\n",
    "# \"647027d011dca20007a66a7a\",\n",
    "# \"64700133cdccc800070ee2cf\",\n",
    "# \"6470324c11dca20007a6e55a\",\n",
    "# \"6470592111dca20007a8a3e4\",\n",
    "# \"6470a16011dca20007ac705b\",\n",
    "# \"6470b42ed49b3a0007515686\",\n",
    "# \"6470da9cd49b3a00075373c7\",\n",
    "# \"64716676fa532900077701a1\",\n",
    "# \"6471744e2687e40007eae331\",\n",
    "# \"64756bb82687e400071917ef\",\n",
    "# \"646c64fceee7ce000767182a\",\n",
    "# \"646c654c4d61f80007b27c02\",\n",
    "# \"646de34d09affa0007cd5079\",\n",
    "# \"646defed4d61f80007c7e165\",\n",
    "# \"646e2e8f09affa0007d2760b\",\n",
    "# \"646eb85f65bbde00070ee93c\",\n",
    "# \"646ed60f65bbde000710303e\",\n",
    "# \"64704bbd9999350007962c15\",\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_imgs = []\n",
    "prompt_masks = []\n",
    "i = 0\n",
    "for image in data[\"images\"]:\n",
    "    if image[\"id\"] in imageid_set and image[\"id\"] not in disclude_id_set:\n",
    "        # req = requests.get(\"http://\" + image[\"url\"])\n",
    "        # img = Image.open(BytesIO(req.content)).convert(\"RGB\")\n",
    "        # prompt_imgs.append(img)\n",
    "        # # prompt_mask = construct_mask(img, image[\"tags\"])\n",
    "        # prompt_mask = generate_binary_mask(image[\"tags\"],img.size)\n",
    "        # prompt_masks.append(prompt_mask)\n",
    "        # print(image[\"id\"])\n",
    "        # out = overlay_segmentation(img,prompt_mask)\n",
    "        # plt.imshow(out)\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.show()\n",
    "        i+=1\n",
    "print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def overlay_segmentation(image, mask):\n",
    "    # Convert the PIL image to a numpy array\n",
    "    image_array = np.array(image)\n",
    "    num_instances = len(np.unique(mask))\n",
    "    # Create a copy of the image array to draw on\n",
    "    overlay = image_array.copy()\n",
    "\n",
    "    # Define colors for each instance ID\n",
    "    colors = generate_equally_spaced_colors(num_instances)\n",
    "\n",
    "    # Draw each instance in a different color on the overlay image\n",
    "    for instance_id in np.unique(mask):\n",
    "        if instance_id == 0:\n",
    "            continue\n",
    "\n",
    "        # Create a binary mask for the current instance ID\n",
    "        instance_mask = np.where(mask == instance_id, 255, 0).astype(np.uint8)\n",
    "\n",
    "        # Find contours in the binary mask\n",
    "        contours, _ = cv2.findContours(instance_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw the contours on the overlay image\n",
    "        cv2.drawContours(overlay, contours, -1, colors[instance_id], thickness=cv2.FILLED)\n",
    "\n",
    "    # Blend the overlay image with the original image\n",
    "    blended_image = cv2.addWeighted(overlay, 0.5, image_array, 0.5, 0)\n",
    "\n",
    "    # Convert the blended image back to PIL format\n",
    "    blended_image_pil = Image.fromarray(blended_image)\n",
    "\n",
    "    return blended_image_pil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seggpt_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_seggpt import LearnablePrompt\n",
    "prompt = LearnablePrompt()\n",
    "\n",
    "prompt.load_state_dict(torch.load(\"bestwires_learned_promptv4.pt\"))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def split_image_into_grids(image, num_horizontal_cells, num_vertical_cells):\n",
    "    width, height = image.size\n",
    "    cell_width = width // num_horizontal_cells\n",
    "    cell_height = height // num_vertical_cells\n",
    "    grids = []\n",
    "    grid_positions = []\n",
    "    for y in range(0, num_vertical_cells):\n",
    "        for x in range(0, num_horizontal_cells):\n",
    "            grid_left = x * cell_width\n",
    "            grid_right = (x + 1) * cell_width\n",
    "            grid_top = y * cell_height\n",
    "            grid_bottom = (y + 1) * cell_height\n",
    "            \n",
    "            if x == num_horizontal_cells - 1:\n",
    "                grid_right = width\n",
    "                \n",
    "            if y == num_vertical_cells - 1:\n",
    "                grid_bottom = height\n",
    "\n",
    "            grid = image.crop((grid_left, grid_top, grid_right, grid_bottom))\n",
    "            grids.append(grid)\n",
    "            grid_positions.append((grid_left,grid_top))\n",
    "    return grids,grid_positions\n",
    "\n",
    "\n",
    "def stitch_masks(image_size, masks,grid_positions):\n",
    "    width, height = image_size\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    for (x, y), m in zip(grid_positions,masks):\n",
    "        mask[y:y + m.shape[0], x:x + m.shape[1]] = m\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "def separate_masks(binary_mask, area_threshold):\n",
    "    # Label connected components in the binary mask\n",
    "    labeled_mask = label(binary_mask)\n",
    "    \n",
    "    # Get region properties of each connected component\n",
    "    regions = regionprops(labeled_mask)\n",
    "    \n",
    "    # Initialize an empty list to store individual masks\n",
    "    separate_masks = []\n",
    "    \n",
    "    # Iterate over each region and create a separate binary mask\n",
    "    for region in regions:\n",
    "        # Filter regions based on area threshold\n",
    "        if region.area >= area_threshold:\n",
    "            instance_mask = (labeled_mask == region.label).astype(np.uint8)\n",
    "            separate_masks.append(instance_mask)\n",
    "    \n",
    "    return separate_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_masks(masks):\n",
    "    # Initialize an empty array to store the combined mask\n",
    "    combined_mask = np.zeros_like(masks[0])\n",
    "\n",
    "    # Assign unique instance IDs to each mask\n",
    "    for i, mask in enumerate(masks, start=1):\n",
    "        # Find the indices where the mask is True\n",
    "        indices = np.where(mask == 1)\n",
    "\n",
    "        # Assign the instance ID to those indices in the combined mask\n",
    "        combined_mask[indices] = i\n",
    "\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def overlay_segmentation(image, mask):\n",
    "    # Convert the PIL image to a numpy array\n",
    "    image_array = np.array(image)\n",
    "    num_instances = len(np.unique(mask))\n",
    "    # Create a copy of the image array to draw on\n",
    "    overlay = image_array.copy()\n",
    "\n",
    "    # Define colors for each instance ID\n",
    "    colors = generate_equally_spaced_colors(num_instances)\n",
    "\n",
    "    # Draw each instance in a different color on the overlay image\n",
    "    for instance_id in np.unique(mask):\n",
    "        if instance_id == 0:\n",
    "            continue\n",
    "\n",
    "        # Create a binary mask for the current instance ID\n",
    "        instance_mask = np.where(mask == instance_id, 255, 0).astype(np.uint8)\n",
    "\n",
    "        # Find contours in the binary mask\n",
    "        contours, _ = cv2.findContours(instance_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Draw the contours on the overlay image\n",
    "        cv2.drawContours(overlay, contours, -1, colors[instance_id], thickness=cv2.FILLED)\n",
    "\n",
    "    # Blend the overlay image with the original image\n",
    "    blended_image = cv2.addWeighted(overlay, 0.5, image_array, 0.5, 0)\n",
    "\n",
    "    # Convert the blended image back to PIL format\n",
    "    blended_image_pil = Image.fromarray(blended_image)\n",
    "\n",
    "    return blended_image_pil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Multilivel!!!\n",
    "# ##Now lets auto-annotate the rest of the images using the single prompt_img/mask\n",
    "# for image in data['images']:\n",
    "#     if image['id'] == \"64baf8d5832640000758762c\": ##Don't predict your prompt_image\n",
    "#         continue \n",
    "#     if image[\"url\"].endswith(\"comundefined\"): ##Matroid Backend Saving Image error\n",
    "#         continue\n",
    "#     req = requests.get(\"http://\" + image[\"url\"])\n",
    "#     test_image = Image.open(BytesIO(req.content)).convert(\"RGB\")\n",
    "#     image_size = test_image.size  \n",
    "#     ##Add Whole Image to Grids to be predicted on   \n",
    "#     test_grids = [test_image]\n",
    "    \n",
    "#     num_horizontal_cells = 4\n",
    "#     num_vertical_cells = 4\n",
    "#     # Split the PIL image into grids\n",
    "#     grids4,grids_pos_4 = split_image_into_grids(test_image, num_horizontal_cells, num_vertical_cells)\n",
    "#     ##Add 4*4 Tiled Image to Grids to be predicted on   \n",
    "#     test_grids = test_grids + grids4\n",
    "    \n",
    "#     num_horizontal_cells = 2\n",
    "#     num_vertical_cells = 2\n",
    "#     # Split the PIL image into grids\n",
    "#     grids2,grids_pos_2 = split_image_into_grids(test_image, num_horizontal_cells, num_vertical_cells)\n",
    "#     ##Add 2*2 Tiled Image to Grids to be predicted on \n",
    "#     test_grids = test_grids + grids2\n",
    "#     # Predict instance segmentation masks for each grid\n",
    "#     masks,_ = seggpt_inference.predict_batch(prompt_img,prompt_mask,test_grids,400)\n",
    "#     # Stitch the masks together to create one segmentation mask\n",
    "#     result_mask4 = stitch_masks(image_size, masks[1:17], grids_pos_4)\n",
    "#     result_mask2 = stitch_masks(image_size, masks[17:], grids_pos_2)\n",
    "#     out_mask = masks[0]\n",
    "#     area_whole = total_segmented_area(out_mask)\n",
    "#     area_grid3 = total_segmented_area(result_mask2)\n",
    "#     area_grid4 = total_segmented_area(result_mask4)\n",
    "#     mask_levels = [out_mask,result_mask2,result_mask4]\n",
    "#     mask_areas = [area_whole,area_grid3,area_grid4]\n",
    "#     # if area_whole > area_grid:\n",
    "#     #     masks = separate_masks(out_mask,30)\n",
    "#     # else:\n",
    "#     #     masks = separate_masks(result_mask,30)\n",
    "#     ##Pick the mask at the most appropriate level\n",
    "#     masks = separate_masks(mask_levels[np.argmax(mask_areas)],30)\n",
    "#     if len(masks) > 0:\n",
    "#         combined_mask = combine_masks(masks)\n",
    "#         mask_overlay = overlay_segmentation(test_image,combined_mask)\n",
    "#     else:\n",
    "#         mask_overlay = test_image\n",
    "    \n",
    "#     # plt.imshow(test_image)\n",
    "#     # plt.show()\n",
    "#     # plt.imshow(out_mask)\n",
    "#     # plt.show()\n",
    "#     # plt.imshow(result_mask2)\n",
    "#     # plt.show()\n",
    "#     # plt.imshow(result_mask4)\n",
    "#     # plt.show()\n",
    "#     plt.imshow(mask_overlay)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def calculate_bounding_boxes(mask):\n",
    "    \"\"\"\n",
    "    Calculate the bounding boxes of objects from a mask where each object has a different instance ID.\n",
    "\n",
    "    Parameters:\n",
    "        mask (numpy.ndarray): Mask where each object has a unique instance ID (integers).\n",
    "\n",
    "    Returns:\n",
    "        List: A list of bounding box tuples [(x_min, y_min, x_max, y_max)] for each object in the mask.\n",
    "    \"\"\"\n",
    "    unique_ids = np.unique(mask)\n",
    "    unique_ids = unique_ids[unique_ids != 0]  # Remove background ID (usually 0)\n",
    "\n",
    "    bounding_boxes = []\n",
    "\n",
    "    for obj_id in unique_ids:\n",
    "        # Find indices of the object with a specific ID\n",
    "        object_indices = np.where(mask == obj_id)\n",
    "\n",
    "        # Calculate minimum and maximum coordinates along x and y axes\n",
    "        x_min = np.min(object_indices[1])\n",
    "        x_max = np.max(object_indices[1])\n",
    "        y_min = np.min(object_indices[0])\n",
    "        y_max = np.max(object_indices[0])\n",
    "\n",
    "        bounding_boxes.append((x_min, y_min, x_max, y_max))\n",
    "\n",
    "    return bounding_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Multilivel!!!\n",
    "# ##Now lets auto-annotate the rest of the images using the single prompt_img/mask\n",
    "# for image in data['images']:\n",
    "#     if image['id'] == \"64baf8d583264000075875da\": ##Don't predict your prompt_image\n",
    "#         continue \n",
    "#     if image[\"url\"].endswith(\"comundefined\"): ##Matroid Backend Saving Image error\n",
    "#         continue\n",
    "#     req = requests.get(\"http://\" + image[\"url\"])\n",
    "#     test_image = Image.open(BytesIO(req.content)).convert(\"RGB\")\n",
    "#     combined_mask = seggpt_inference.predict_tiled(prompt_img,prompt_mask,test_image)\n",
    "#     if combined_mask is not None:\n",
    "#         bboxes = calculate_bounding_boxes(combined_mask)\n",
    "#         mask_overlay = overlay_segmentation(test_image,combined_mask)\n",
    "#         plt.imshow(mask_overlay)\n",
    "#         for (x_min,y_min,x_max,y_max) in bboxes:\n",
    "#             plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n",
    "#                                         linewidth=0.5, edgecolor='r', facecolor='none'))\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         mask_overlay = test_image\n",
    "#         plt.imshow(mask_overlay)\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_mask(image, mask):\n",
    "  \"\"\"\n",
    "  Overlays a binary mask on a PIL image.\n",
    "  \n",
    "  Args:\n",
    "    image: PIL Image to overlay mask on\n",
    "    mask: 2D numpy array with shape (H, W) containing binary mask\n",
    "  \"\"\"\n",
    "  \n",
    "  # Convert mask to PIL Image\n",
    "  mask = Image.fromarray(mask.astype(np.uint8)*255)\n",
    "  mask = mask.resize(image.size)\n",
    "  \n",
    "  # Convert to RGBA\n",
    "  mask = mask.convert('RGBA')\n",
    "  mask = np.array(mask)\n",
    "  \n",
    "  # Overlay mask red channel onto RGB image \n",
    "  image = np.array(image) \n",
    "  image[:,:,0] = np.where(mask[:,:,3] > 0, mask[:,:,0], image[:,:,0])\n",
    "  \n",
    "  return Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_img,prompt_mask = prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_miou(pred, gt):\n",
    "\n",
    "  classes = np.unique(gt)\n",
    "  classes = classes[classes != 0]\n",
    "  if len(classes) == 0:\n",
    "    return 0 # or nan/other default value\n",
    "  iou_list = []\n",
    "\n",
    "  for c in classes:\n",
    "    pred_c = (pred == c)   \n",
    "    gt_c = (gt == c)\n",
    "\n",
    "    intersection = np.logical_and(pred_c, gt_c).sum()\n",
    "    union = np.logical_or(pred_c, gt_c).sum()\n",
    "    eps = 1e-6\n",
    "    iou = intersection / (union + eps)\n",
    "    iou_list.append(iou)\n",
    "\n",
    "  miou = np.mean(iou_list)\n",
    "  return miou "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m test_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(BytesIO(req\u001b[39m.\u001b[39mcontent))\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# combined_mask = seggpt_inference.predict_tiled_finetuned(prompt_img,prompt_mask,test_image)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m masks,level \u001b[39m=\u001b[39m seggpt_inference\u001b[39m.\u001b[39;49mpredict_tiled_finetuned(prompt_img,prompt_mask,test_image)    \n\u001b[1;32m     16\u001b[0m mask_overlay \u001b[39m=\u001b[39m overlay_mask(test_image,masks[level])\n\u001b[1;32m     17\u001b[0m levels \u001b[39m=\u001b[39m {\u001b[39m0\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39mOriginal\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m1\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39m2x2\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m2\u001b[39m:\u001b[39m\"\u001b[39m\u001b[39m4x4\u001b[39m\u001b[39m\"\u001b[39m}\n",
      "File \u001b[0;32m~/SegmentationFineTune/seggpt/seggpt_inference.py:522\u001b[0m, in \u001b[0;36mpredict_tiled_finetuned\u001b[0;34m(prompt_image, prompt_mask, test_image, seperate, num_cats)\u001b[0m\n\u001b[1;32m    520\u001b[0m test_grids \u001b[39m=\u001b[39m test_grids \u001b[39m+\u001b[39m grids2\n\u001b[1;32m    521\u001b[0m \u001b[39m# Predict instance segmentation masks for each grid\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m masks,_ \u001b[39m=\u001b[39m predict_batch_finetuned(prompt_image,prompt_mask,test_grids,\u001b[39m400\u001b[39;49m,num_cats)\n\u001b[1;32m    523\u001b[0m \u001b[39m# Stitch the masks together to create one segmentation mask\u001b[39;00m\n\u001b[1;32m    524\u001b[0m result_mask4 \u001b[39m=\u001b[39m stitch_masks(image_size, masks[\u001b[39m1\u001b[39m:\u001b[39m17\u001b[39m], grids_pos_4)\n",
      "File \u001b[0;32m~/SegmentationFineTune/seggpt/seggpt_inference.py:344\u001b[0m, in \u001b[0;36mpredict_batch_finetuned\u001b[0;34m(prompt_img, prompt_mask, test_imgs, threshold, num_cats)\u001b[0m\n\u001b[1;32m    342\u001b[0m     cat_to_color[i] \u001b[39m=\u001b[39m colors[i]\n\u001b[1;32m    343\u001b[0m color_to_cat \u001b[39m=\u001b[39m {v: k \u001b[39mfor\u001b[39;00m (k, v) \u001b[39min\u001b[39;00m cat_to_color\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 344\u001b[0m outputs \u001b[39m=\u001b[39m inference_image_batch_finetuned(\n\u001b[1;32m    345\u001b[0m         model, device, prompt_img, prompt_mask, test_imgs\n\u001b[1;32m    346\u001b[0m     )\n\u001b[1;32m    347\u001b[0m output_masks \u001b[39m=\u001b[39m []\n\u001b[1;32m    348\u001b[0m \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs:\n",
      "File \u001b[0;32m~/SegmentationFineTune/seggpt/seggpt_inference.py:272\u001b[0m, in \u001b[0;36minference_image_batch_finetuned\u001b[0;34m(model, device, prompt_img, prompt_mask, test_imgs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    269\u001b[0m     \u001b[39mwith\u001b[39;00m ctx:\n\u001b[1;32m    270\u001b[0m                     loss, y, mask \u001b[39m=\u001b[39m model(\n\u001b[1;32m    271\u001b[0m                         x\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device),\n\u001b[0;32m--> 272\u001b[0m                         tgt\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39mto(device),\n\u001b[1;32m    273\u001b[0m                         bool_masked_pos,\n\u001b[1;32m    274\u001b[0m                         valid,\n\u001b[1;32m    275\u001b[0m                         seg_type,\n\u001b[1;32m    276\u001b[0m                         feat_ensemble,\n\u001b[1;32m    277\u001b[0m                     )\n\u001b[1;32m    278\u001b[0m y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39munpatchify(y)\n\u001b[1;32m    279\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mnchw->nhwc\u001b[39m\u001b[39m\"\u001b[39m, y)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##Multilivel!!!\n",
    "##Now lets auto-annotate the rest of the images using the single prompt_img/mask\n",
    "count = 0\n",
    "mIoU_no_tiling = []\n",
    "mIoU_2_2 = []\n",
    "mIoU_4_4 = []\n",
    "mIoU_combined = []\n",
    "for image in data['images']:\n",
    "    if image[\"url\"].endswith(\"comundefined\"): ##Matroid Backend Saving Image error\n",
    "        continue \n",
    "    count += 1\n",
    "    req = requests.get(\"http://\" + image[\"url\"])\n",
    "    test_image = Image.open(BytesIO(req.content)).convert(\"RGB\")\n",
    "    # combined_mask = seggpt_inference.predict_tiled_finetuned(prompt_img,prompt_mask,test_image)\n",
    "    masks,level = seggpt_inference.predict_tiled_finetuned(prompt_img,prompt_mask,test_image)    \n",
    "    mask_overlay = overlay_mask(test_image,masks[level])\n",
    "    levels = {0:\"Original\",1:\"2x2\",2:\"4x4\"}\n",
    "    gt_mask = generate_binary_mask(image[\"tags\"],test_image.size)\n",
    "    mIoU_no_tiling.append(calculate_miou(masks[0],gt_mask))\n",
    "    mIoU_2_2.append(calculate_miou(masks[1],gt_mask))\n",
    "    mIoU_4_4.append(calculate_miou(masks[2],gt_mask))\n",
    "    mIoU_combined.append(calculate_miou(masks[level],gt_mask))\n",
    "    # plt.imshow(mask_overlay)\n",
    "    # plt.title(\"Overlayed Image\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    # plt.imshow(gt_mask)\n",
    "    # plt.title(\"GT mask\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    # plt.imshow(masks[0])\n",
    "    # plt.title(\"No Tiling Mask\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    # plt.imshow(masks[1])\n",
    "    # plt.title(\"2x2 Tiling Mask\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    # plt.imshow(masks[2])\n",
    "    # plt.title(\"4x4 Tiling Mask\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n",
    "    # print(f\"Chosen Level: {levels[level]}\")\n",
    "    # i += 1\n",
    "    # if i == 5:\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7820374339156037"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mIoU_4_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7836701612463659"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mIoU_2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6730860110201617"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mIoU_no_tiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7845496576657801"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mIoU_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SegGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
